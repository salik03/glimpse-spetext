{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suNrxoS3Yzsg"
      },
      "source": [
        "# 1. Import and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7nNfHD0Yzsj",
        "outputId": "b953424d-6a61-477e-e99d-f1a44742b449"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hmOR7jBQYzsk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1fVWc6E1ES3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0yE526pYzsl"
      },
      "source": [
        "# 2. Keypoints using MP Holistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dtZfMQPIYzsl"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FRQMLB1TYzsl"
      },
      "outputs": [],
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "boMUyxEFYzsm"
      },
      "outputs": [],
      "source": [
        "def draw_landmarks(image, results):\n",
        "    #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2O94BtXoYzsm"
      },
      "outputs": [],
      "source": [
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS,\n",
        "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "    #                          )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XMQ4z7J3Yzsn"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpatches\u001b[39;00m \u001b[39mimport\u001b[39;00m cv2_imshow\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m'\u001b[39m\u001b[39m/content/vid.mp4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Set mediapipe model\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# cap = cv2.VideoCapture('/content/vid.mp4')\n",
        "# # Set mediapipe model\n",
        "# holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "# while cap.isOpened():\n",
        "\n",
        "#     # Read feed\n",
        "#     ret, frame = cap.read()\n",
        "\n",
        "#     if not ret:\n",
        "#         break\n",
        "\n",
        "#     # Make detections\n",
        "#     image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "#     # Draw landmarks\n",
        "#     draw_styled_landmarks(image, results)\n",
        "\n",
        "#     # Show to screen\n",
        "#     #cv2_imshow(image)\n",
        "#     print(results.left_hand_landmarks.landmark)\n",
        "\n",
        "#     # Break gracefully\n",
        "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "#         break\n",
        "\n",
        "# cap.release()\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPYZMaHGYzso"
      },
      "source": [
        "# 3. Extract Keypoint Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttI3dgmCYzso",
        "outputId": "a82432ad-5542-42ef-e20a-e4e49b19c1ee"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mleft_hand_landmarks:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     num_landmarks \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(results\u001b[39m.\u001b[39mleft_hand_landmarks\u001b[39m.\u001b[39mlandmark)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of detected landmarks for left hand: \u001b[39m\u001b[39m{\u001b[39;00mnum_landmarks\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "# if results.left_hand_landmarks:\n",
        "#     num_landmarks = len(results.left_hand_landmarks.landmark)\n",
        "#     print(f\"Number of detected landmarks for left hand: {num_landmarks}\")\n",
        "# else:\n",
        "#     print(\"No landmarks detected for the left hand in this frame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6YSvXScfYzso"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pose \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([res\u001b[39m.\u001b[39mx, res\u001b[39m.\u001b[39my, res\u001b[39m.\u001b[39mz, res\u001b[39m.\u001b[39mvisibility])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pose\u001b[39m.\u001b[39mappend(test)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "# pose = []\n",
        "# for res in results.pose_landmarks.landmark:\n",
        "#     test = np.array([res.x, res.y, res.z, res.visibility])\n",
        "#     pose.append(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RezjcpGxYzso"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pose \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[res\u001b[39m.\u001b[39mx, res\u001b[39m.\u001b[39my, res\u001b[39m.\u001b[39mz, res\u001b[39m.\u001b[39mvisibility] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark])\u001b[39m.\u001b[39mflatten() \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mzeros(\u001b[39m132\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m face \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[res\u001b[39m.\u001b[39mx, res\u001b[39m.\u001b[39my, res\u001b[39m.\u001b[39mz] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mface_landmarks\u001b[39m.\u001b[39mlandmark])\u001b[39m.\u001b[39mflatten() \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mface_landmarks \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mzeros(\u001b[39m1404\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[res\u001b[39m.\u001b[39mx, res\u001b[39m.\u001b[39my, res\u001b[39m.\u001b[39mz] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mleft_hand_landmarks\u001b[39m.\u001b[39mlandmark])\u001b[39m.\u001b[39mflatten() \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mleft_hand_landmarks \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mzeros(\u001b[39m21\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "# pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
        "# face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
        "# lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "# rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sH-IX0XMYzso"
      },
      "outputs": [],
      "source": [
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lh, rh])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fbKWeHY0Yzsp"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result_test \u001b[39m=\u001b[39m extract_keypoints(results)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "# result_test = extract_keypoints(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT3XGSvWYzsp",
        "outputId": "ec018ec4-f5ee-4b1d-c8fd-bc9d3021d518"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'result_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result_test\n",
            "\u001b[1;31mNameError\u001b[0m: name 'result_test' is not defined"
          ]
        }
      ],
      "source": [
        "# result_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IuhvPXuQYzsp"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'result_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m, result_test)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'result_test' is not defined"
          ]
        }
      ],
      "source": [
        "# np.save('0', result_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMq5M1P9Yzsp",
        "outputId": "9f037965-5a78-49ea-dab5-feadd5afdb03"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '0.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m0.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0.npy'"
          ]
        }
      ],
      "source": [
        "# np.load('0.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5_o6xo7Yzsp"
      },
      "source": [
        "# 4. Setup Folders for Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6Mmp0XDbYzsp"
      },
      "outputs": [],
      "source": [
        "# Path for exported data, numpy arrays\n",
        "DATA_PATH = os.path.join('MP_Data')\n",
        "\n",
        "# Actions that we try to detect\n",
        "actions = np.array([\n",
        "    \"Able\",\n",
        "    \"About\",\n",
        "    \"Above\",\n",
        "    \"Accept (2)\",\n",
        "    \"Accept\",\n",
        "    \"Accident\",\n",
        "    \"Acquire\",\n",
        "    \"Across)\",\n",
        "    \"Act\",\n",
        "    \"Active\",\n",
        "    \"Actor\",\n",
        "    \"Addition\",\n",
        "    \"Address\",\n",
        "    \"Admonish\",\n",
        "    \"Adolescent\",\n",
        "    \"Adopt\",\n",
        "    \"Advocate\",\n",
        "    \"Afraid\",\n",
        "    \"After\",\n",
        "    \"Afterward\",\n",
        "    \"Again\",\n",
        "    \"Against\",\n",
        "    \"Age\",\n",
        "    \"Agree\",\n",
        "    \"Aid\",\n",
        "    \"Airplane\",\n",
        "    \"Aisle\",\n",
        "    \"Alarm\",\n",
        "    \"Alcohol\",\n",
        "    \"Alive\",\n",
        "    \"All\",\n",
        "    \"Alleluia\",\n",
        "    \"Allergies\",\n",
        "    \"Alligator\",\n",
        "    \"Allow\",\n",
        "    \"Almighty\",\n",
        "    \"Almost\",\n",
        "    \"Alone\",\n",
        "    \"Alright\",\n",
        "    \"Altar\",\n",
        "    \"Always\",\n",
        "    \"And\",\n",
        "    \"Angel\",\n",
        "    \"Anger\",\n",
        "    \"Angry\",\n",
        "    \"Animal\",\n",
        "    \"Announce\",\n",
        "    \"Announcement\",\n",
        "    \"Annunciation\",\n",
        "    \"Answer\",\n",
        "    \"Anthem\",\n",
        "    \"Anxiety\",\n",
        "    \"Any\",\n",
        "    \"Anyone\",\n",
        "    \"Anyway\",\n",
        "    \"Ape\",\n",
        "    \"Apology\",\n",
        "    \"Apostasy\",\n",
        "    \"Apostle\",\n",
        "    \"Appear\",\n",
        "    \"Applause\",\n",
        "    \"Apple\",\n",
        "    \"Appoint\",\n",
        "    \"Archbishop\",\n",
        "    \"Archdiocese\",\n",
        "    \"Area\",\n",
        "    \"Argue\",\n",
        "    \"Arise\",\n",
        "    \"Arrive\",\n",
        "    \"Art\",\n",
        "    \"Article\",\n",
        "    \"Ascend\",\n",
        "    \"Ascension\",\n",
        "    \"Ask\",\n",
        "    \"Aspergillum\",\n",
        "    \"Assemble\",\n",
        "    \"Assembly\",\n",
        "    \"Assist\",\n",
        "    \"Associate\",\n",
        "    \"Association\",\n",
        "    \"Assume\",\n",
        "    \"Assumption\",\n",
        "    \"Astonish\",\n",
        "    \"Astray\",\n",
        "    \"Atone\",\n",
        "    \"Atonement\",\n",
        "    \"Attend\",\n",
        "    \"Aunt\",\n",
        "    \"Authority\",\n",
        "    \"Average\",\n",
        "    \"Bathroom\",\n",
        "    \"Beautiful\",\n",
        "    \"Busy\",\n",
        "    \"Connect)\",\n",
        "    \"Done\",\n",
        "    \"God\",\n",
        "    \"Other\",\n",
        "    \"Plus\",\n",
        "    \"Space)\",\n",
        "    \"Space\",\n",
        "    \"To)\",\n",
        "    \"Wednesday\"\n",
        "]\n",
        ")\n",
        "\n",
        "# Thirty videos worth of data\n",
        "no_sequences = 50\n",
        "\n",
        "# Videos are going to be 30 frames in length\n",
        "sequence_length = 30\n",
        "\n",
        "# Folder start\n",
        "start_folder = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Desktop\\Glimpse\\Backend\\speechtogif\\Action_Detection_Refined.ipynb Cell 23\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#Y121sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         np\u001b[39m.\u001b[39msave(npy_path, keypoints)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#Y121sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m         \u001b[39m# Break gracefully\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#Y121sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m         \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m10\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#Y121sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Desktop/Glimpse/Backend/speechtogif/Action_Detection_Refined.ipynb#Y121sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m cap\u001b[39m.\u001b[39mrelease()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import mediapipe as mp\n",
        "\n",
        "# # Function to perform landmark detection using MediaPipe\n",
        "# def mediapipe_detection(image, model):\n",
        "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "#     image.flags.writeable = False                  # Image is no longer writeable\n",
        "#     results = model.process(image)                 # Make prediction\n",
        "#     image.flags.writeable = True                   # Image is now writeable\n",
        "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "#     return image, results\n",
        "\n",
        "# # Function to draw landmarks on the image\n",
        "# def draw_styled_landmarks(image, results):\n",
        "#     # Draw face connections\n",
        "#     # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS,\n",
        "#     #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "#     #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "#     #                          )\n",
        "#     # Draw pose connections\n",
        "#     mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "#                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "#                              mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "#                              )\n",
        "#     # Draw left hand connections\n",
        "#     mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "#                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "#                              mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "#                              )\n",
        "#     # Draw right hand connections\n",
        "#     mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "#                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "#                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "#                              )\n",
        "\n",
        "# # Function to extract keypoints from detection results\n",
        "# def extract_keypoints(results):\n",
        "#     pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "#     lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "#     rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "#     return np.concatenate([pose, lh, rh])\n",
        "\n",
        "# # Path for the folder containing sign language videos\n",
        "# VIDEO_FOLDER_PATH = 'finaldataset'\n",
        "\n",
        "# # Path for exported data, numpy arrays\n",
        "# DATA_PATH = os.path.join('MP_Data')\n",
        "\n",
        "# # Actions that we try to detect\n",
        "\n",
        "# # Thirty frames worth of data\n",
        "# sequence_length = 30\n",
        "\n",
        "# # Loop through each video file in the specified folder\n",
        "# for video_file in os.listdir(VIDEO_FOLDER_PATH):\n",
        "#     # Construct the full path of the video file\n",
        "#     video_path = os.path.join(VIDEO_FOLDER_PATH, video_file)\n",
        "\n",
        "#     # Set up video capture\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "#     # Set up MediaPipe model\n",
        "#     with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "#         # Extract the video file name (without extension) for creating the output folder\n",
        "#         file_name = os.path.splitext(video_file)[0]\n",
        "\n",
        "#         # Create a folder for each video file\n",
        "#         output_folder_path = os.path.join(DATA_PATH, file_name)\n",
        "#         os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "#         # Loop through video frames\n",
        "#         for frame_num in range(sequence_length):\n",
        "#             ret, frame = cap.read()\n",
        "\n",
        "#             # Make detections\n",
        "#             image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "#             # Draw landmarks\n",
        "#             draw_styled_landmarks(image, results)\n",
        "\n",
        "#             # Display message\n",
        "#             cv2.putText(image, 'Collecting frames for Video: {}'.format(file_name), (15, 12),\n",
        "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "#             cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "#             # Export keypoints\n",
        "#             keypoints = extract_keypoints(results)\n",
        "#             npy_path = os.path.join(output_folder_path, str(frame_num))\n",
        "#             np.save(npy_path, keypoints)\n",
        "\n",
        "#             # Break gracefully\n",
        "#             if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "#                 break\n",
        "\n",
        "#     cap.release()\n",
        "#     cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "-VCQtrg-Yzsp",
        "outputId": "ce0d1bc7-a4d8-47f3-ce47-56a8884be78a"
      },
      "outputs": [],
      "source": [
        "# for action in actions:\n",
        "#     dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n",
        "#     for sequence in range(1,no_sequences+1):\n",
        "#         try:\n",
        "#             os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n",
        "#         except:\n",
        "#             pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z6BvxoLYzsp"
      },
      "source": [
        "# 5. Collect Keypoint Values for Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXNovAkiYzsp"
      },
      "outputs": [],
      "source": [
        "# cap = cv2.VideoCapture(0)\n",
        "# # Set mediapipe model\n",
        "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "#     # NEW LOOP\n",
        "#     # Loop through actions\n",
        "#     for action in actions:\n",
        "#         # Loop through sequences aka videos\n",
        "#         for sequence in range(start_folder, start_folder+no_sequences):\n",
        "#             # Loop through video length aka sequence length\n",
        "#             for frame_num in range(sequence_length):\n",
        "\n",
        "#                 # Read feed\n",
        "#                 ret, frame = cap.read()\n",
        "\n",
        "#                 # Make detections\n",
        "#                 image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "#                 # Draw landmarks\n",
        "#                 draw_styled_landmarks(image, results)\n",
        "\n",
        "#                 # NEW Apply wait logic\n",
        "#                 if frame_num == 0:\n",
        "#                     cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
        "#                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
        "#                     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "#                     # Show to screen\n",
        "#                     cv2.imshow('OpenCV Feed', image)\n",
        "#                     cv2.waitKey(500)\n",
        "#                 else:\n",
        "#                     cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
        "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "#                     # Show to screen\n",
        "#                     cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "#                 # NEW Export keypoints\n",
        "#                 keypoints = extract_keypoints(results)\n",
        "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
        "#                 np.save(npy_path, keypoints)\n",
        "\n",
        "#                 # Break gracefully\n",
        "#                 if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "#                     break\n",
        "\n",
        "#     cap.release()\n",
        "#     cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfgPmcPRYzsp"
      },
      "outputs": [],
      "source": [
        "# cap.release()\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4-EFMOkYzsp"
      },
      "source": [
        "# 6. Preprocess Data and Create Labels and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "buYhaf4MYzsp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Gwx3gmYrYzsq"
      },
      "outputs": [],
      "source": [
        "label_map = {label:num for num, label in enumerate(actions)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ImbrFKHeYzsq",
        "outputId": "e4a9d629-e090-4f71-d1a3-3e7b51ce7358"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Able': 0,\n",
              " 'About': 1,\n",
              " 'Above': 2,\n",
              " 'Accept (2)': 3,\n",
              " 'Accept': 4,\n",
              " 'Accident': 5,\n",
              " 'Acquire': 6,\n",
              " 'Across)': 7,\n",
              " 'Act': 8,\n",
              " 'Active': 9,\n",
              " 'Actor': 10,\n",
              " 'Addition': 11,\n",
              " 'Address': 12,\n",
              " 'Admonish': 13,\n",
              " 'Adolescent': 14,\n",
              " 'Adopt': 15,\n",
              " 'Advocate': 16,\n",
              " 'Afraid': 17,\n",
              " 'After': 18,\n",
              " 'Afterward': 19,\n",
              " 'Again': 20,\n",
              " 'Against': 21,\n",
              " 'Age': 22,\n",
              " 'Agree': 23,\n",
              " 'Aid': 24,\n",
              " 'Airplane': 25,\n",
              " 'Aisle': 26,\n",
              " 'Alarm': 27,\n",
              " 'Alcohol': 28,\n",
              " 'Alive': 29,\n",
              " 'All': 30,\n",
              " 'Alleluia': 31,\n",
              " 'Allergies': 32,\n",
              " 'Alligator': 33,\n",
              " 'Allow': 34,\n",
              " 'Almighty': 35,\n",
              " 'Almost': 36,\n",
              " 'Alone': 37,\n",
              " 'Alright': 38,\n",
              " 'Altar': 39,\n",
              " 'Always': 40,\n",
              " 'And': 41,\n",
              " 'Angel': 42,\n",
              " 'Anger': 43,\n",
              " 'Angry': 44,\n",
              " 'Animal': 45,\n",
              " 'Announce': 46,\n",
              " 'Announcement': 47,\n",
              " 'Annunciation': 48,\n",
              " 'Answer': 49,\n",
              " 'Anthem': 50,\n",
              " 'Anxiety': 51,\n",
              " 'Any': 52,\n",
              " 'Anyone': 53,\n",
              " 'Anyway': 54,\n",
              " 'Ape': 55,\n",
              " 'Apology': 56,\n",
              " 'Apostasy': 57,\n",
              " 'Apostle': 58,\n",
              " 'Appear': 59,\n",
              " 'Applause': 60,\n",
              " 'Apple': 61,\n",
              " 'Appoint': 62,\n",
              " 'Archbishop': 63,\n",
              " 'Archdiocese': 64,\n",
              " 'Area': 65,\n",
              " 'Argue': 66,\n",
              " 'Arise': 67,\n",
              " 'Arrive': 68,\n",
              " 'Art': 69,\n",
              " 'Article': 70,\n",
              " 'Ascend': 71,\n",
              " 'Ascension': 72,\n",
              " 'Ask': 73,\n",
              " 'Aspergillum': 74,\n",
              " 'Assemble': 75,\n",
              " 'Assembly': 76,\n",
              " 'Assist': 77,\n",
              " 'Associate': 78,\n",
              " 'Association': 79,\n",
              " 'Assume': 80,\n",
              " 'Assumption': 81,\n",
              " 'Astonish': 82,\n",
              " 'Astray': 83,\n",
              " 'Atone': 84,\n",
              " 'Atonement': 85,\n",
              " 'Attend': 86,\n",
              " 'Aunt': 87,\n",
              " 'Authority': 88,\n",
              " 'Average': 89,\n",
              " 'Bathroom': 90,\n",
              " 'Beautiful': 91,\n",
              " 'Busy': 92,\n",
              " 'Connect)': 93,\n",
              " 'Done': 94,\n",
              " 'God': 95,\n",
              " 'Other': 96,\n",
              " 'Plus': 97,\n",
              " 'Space)': 98,\n",
              " 'Space': 99,\n",
              " 'To)': 100,\n",
              " 'Wednesday': 101}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "X5gHNhBNYzsq"
      },
      "outputs": [],
      "source": [
        "sequences, labels = [], []\n",
        "for action in actions:\n",
        "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))):\n",
        "        window = []\n",
        "        for frame_num in range(sequence_length):\n",
        "            res = np.load(os.path.join(DATA_PATH, action, \"{}.npy\".format(frame_num)))\n",
        "            window.append(res)\n",
        "        sequences.append(window)\n",
        "        labels.append(label_map[action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-idzJ0xNYzsq",
        "outputId": "88583eae-551e-4297-d093-723a04f29423"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3060, 30, 258)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(sequences).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "wSp6O0W4Yzsq",
        "outputId": "77dea97e-6e21-467b-adb8-389d4622f345"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3060,)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(labels).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DrsJZwgKYzsq"
      },
      "outputs": [],
      "source": [
        "X = np.array(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ccXihY-uYzsq",
        "outputId": "86bad1d9-85ed-42b3-94fa-19027f893bba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3060, 30, 258)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "J7IzDKnTYzsq"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(labels).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Jb8phQmbYzsq"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5zllzFByYzsq",
        "outputId": "9a8cff08-c03c-42ab-bee5-9e53471ac261"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(153, 102)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exblJ2hRYzsv"
      },
      "source": [
        "# 7. Build and Train LSTM Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ZNeAtzeCYzsv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "r38sM-rwYzsv"
      },
      "outputs": [],
      "source": [
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "yCPjnyscYzsv"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BdVEfHqAYzsw"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "N1vmAYVvYzsw",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "91/91 [==============================] - 6s 28ms/step - loss: 4.6107 - categorical_accuracy: 0.0138\n",
            "Epoch 2/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 4.5670 - categorical_accuracy: 0.0241\n",
            "Epoch 3/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 3.9656 - categorical_accuracy: 0.0458\n",
            "Epoch 4/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 3.0750 - categorical_accuracy: 0.1441\n",
            "Epoch 5/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 2.2661 - categorical_accuracy: 0.2804\n",
            "Epoch 6/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 1.5871 - categorical_accuracy: 0.4599\n",
            "Epoch 7/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 1.1734 - categorical_accuracy: 0.5886\n",
            "Epoch 8/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.8804 - categorical_accuracy: 0.7021\n",
            "Epoch 9/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.5555 - categorical_accuracy: 0.7970\n",
            "Epoch 10/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.4549 - categorical_accuracy: 0.8369\n",
            "Epoch 11/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.3544 - categorical_accuracy: 0.8717\n",
            "Epoch 12/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.3727 - categorical_accuracy: 0.8789\n",
            "Epoch 13/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.3502 - categorical_accuracy: 0.8751\n",
            "Epoch 14/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2081 - categorical_accuracy: 0.9154\n",
            "Epoch 15/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2268 - categorical_accuracy: 0.9223\n",
            "Epoch 16/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.4576 - categorical_accuracy: 0.8610\n",
            "Epoch 17/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2263 - categorical_accuracy: 0.9288\n",
            "Epoch 18/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1293 - categorical_accuracy: 0.9374\n",
            "Epoch 19/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.1272 - categorical_accuracy: 0.9401\n",
            "Epoch 20/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1402 - categorical_accuracy: 0.9446\n",
            "Epoch 21/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1050 - categorical_accuracy: 0.9456\n",
            "Epoch 22/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.4481 - categorical_accuracy: 0.8548\n",
            "Epoch 23/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1310 - categorical_accuracy: 0.9364\n",
            "Epoch 24/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1033 - categorical_accuracy: 0.9484\n",
            "Epoch 25/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1076 - categorical_accuracy: 0.9549\n",
            "Epoch 26/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0887 - categorical_accuracy: 0.9508\n",
            "Epoch 27/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0869 - categorical_accuracy: 0.9522\n",
            "Epoch 28/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2636 - categorical_accuracy: 0.9095\n",
            "Epoch 29/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0851 - categorical_accuracy: 0.9498\n",
            "Epoch 30/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0853 - categorical_accuracy: 0.9549\n",
            "Epoch 31/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0852 - categorical_accuracy: 0.9515\n",
            "Epoch 32/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.3015 - categorical_accuracy: 0.8961\n",
            "Epoch 33/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1551 - categorical_accuracy: 0.9346\n",
            "Epoch 34/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1447 - categorical_accuracy: 0.9353\n",
            "Epoch 35/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0837 - categorical_accuracy: 0.9494\n",
            "Epoch 36/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0892 - categorical_accuracy: 0.9512\n",
            "Epoch 37/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0802 - categorical_accuracy: 0.9539\n",
            "Epoch 38/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0776 - categorical_accuracy: 0.9539\n",
            "Epoch 39/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.0794 - categorical_accuracy: 0.9549\n",
            "Epoch 40/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0814 - categorical_accuracy: 0.9542\n",
            "Epoch 41/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0853 - categorical_accuracy: 0.9556\n",
            "Epoch 42/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0739 - categorical_accuracy: 0.9591\n",
            "Epoch 43/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1062 - categorical_accuracy: 0.9446\n",
            "Epoch 44/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0793 - categorical_accuracy: 0.9556\n",
            "Epoch 45/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.9973 - categorical_accuracy: 0.7348\n",
            "Epoch 46/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.4012 - categorical_accuracy: 0.8700\n",
            "Epoch 47/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1118 - categorical_accuracy: 0.9508\n",
            "Epoch 48/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0876 - categorical_accuracy: 0.9536\n",
            "Epoch 49/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.0968 - categorical_accuracy: 0.9470\n",
            "Epoch 50/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.0779 - categorical_accuracy: 0.9573\n",
            "Epoch 51/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0766 - categorical_accuracy: 0.9546\n",
            "Epoch 52/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0799 - categorical_accuracy: 0.9525\n",
            "Epoch 53/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0865 - categorical_accuracy: 0.9491\n",
            "Epoch 54/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1095 - categorical_accuracy: 0.9450\n",
            "Epoch 55/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2122 - categorical_accuracy: 0.9150\n",
            "Epoch 56/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0978 - categorical_accuracy: 0.9481\n",
            "Epoch 57/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0747 - categorical_accuracy: 0.9539\n",
            "Epoch 58/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0789 - categorical_accuracy: 0.9525\n",
            "Epoch 59/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0759 - categorical_accuracy: 0.9563\n",
            "Epoch 60/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0766 - categorical_accuracy: 0.9563\n",
            "Epoch 61/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2141 - categorical_accuracy: 0.9116\n",
            "Epoch 62/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1234 - categorical_accuracy: 0.9426\n",
            "Epoch 63/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0792 - categorical_accuracy: 0.9525\n",
            "Epoch 64/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0773 - categorical_accuracy: 0.9549\n",
            "Epoch 65/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0732 - categorical_accuracy: 0.9594\n",
            "Epoch 66/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0731 - categorical_accuracy: 0.9598\n",
            "Epoch 67/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.3145 - categorical_accuracy: 0.8947\n",
            "Epoch 68/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0818 - categorical_accuracy: 0.9598\n",
            "Epoch 69/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0688 - categorical_accuracy: 0.9591\n",
            "Epoch 70/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0671 - categorical_accuracy: 0.9615\n",
            "Epoch 71/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0649 - categorical_accuracy: 0.9622\n",
            "Epoch 72/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0711 - categorical_accuracy: 0.9587\n",
            "Epoch 73/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0676 - categorical_accuracy: 0.9591\n",
            "Epoch 74/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0588 - categorical_accuracy: 0.9670\n",
            "Epoch 75/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0511 - categorical_accuracy: 0.9649\n",
            "Epoch 76/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1157 - categorical_accuracy: 0.9470\n",
            "Epoch 77/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.3639 - categorical_accuracy: 0.8734\n",
            "Epoch 78/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0720 - categorical_accuracy: 0.9567\n",
            "Epoch 79/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0622 - categorical_accuracy: 0.9653\n",
            "Epoch 80/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0589 - categorical_accuracy: 0.9632\n",
            "Epoch 81/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0694 - categorical_accuracy: 0.9570\n",
            "Epoch 82/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0571 - categorical_accuracy: 0.9625\n",
            "Epoch 83/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0559 - categorical_accuracy: 0.9663\n",
            "Epoch 84/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.5916 - categorical_accuracy: 0.8356\n",
            "Epoch 85/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0884 - categorical_accuracy: 0.9563\n",
            "Epoch 86/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0593 - categorical_accuracy: 0.9666\n",
            "Epoch 87/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0529 - categorical_accuracy: 0.9690\n",
            "Epoch 88/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1349 - categorical_accuracy: 0.9532\n",
            "Epoch 89/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1793 - categorical_accuracy: 0.9401\n",
            "Epoch 90/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0486 - categorical_accuracy: 0.9711\n",
            "Epoch 91/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0447 - categorical_accuracy: 0.9680\n",
            "Epoch 92/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0432 - categorical_accuracy: 0.9684\n",
            "Epoch 93/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0432 - categorical_accuracy: 0.9749\n",
            "Epoch 94/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0451 - categorical_accuracy: 0.9694\n",
            "Epoch 95/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0429 - categorical_accuracy: 0.9687\n",
            "Epoch 96/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0425 - categorical_accuracy: 0.9708\n",
            "Epoch 97/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0435 - categorical_accuracy: 0.9656\n",
            "Epoch 98/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0444 - categorical_accuracy: 0.9714\n",
            "Epoch 99/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0435 - categorical_accuracy: 0.9690\n",
            "Epoch 100/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0423 - categorical_accuracy: 0.9708\n",
            "Epoch 101/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0410 - categorical_accuracy: 0.9721\n",
            "Epoch 102/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0423 - categorical_accuracy: 0.9701\n",
            "Epoch 103/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 1.2641 - categorical_accuracy: 0.6766\n",
            "Epoch 104/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1768 - categorical_accuracy: 0.9405\n",
            "Epoch 105/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0599 - categorical_accuracy: 0.9680\n",
            "Epoch 106/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0498 - categorical_accuracy: 0.9677\n",
            "Epoch 107/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0437 - categorical_accuracy: 0.9728\n",
            "Epoch 108/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0419 - categorical_accuracy: 0.9701\n",
            "Epoch 109/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0420 - categorical_accuracy: 0.9708\n",
            "Epoch 110/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0425 - categorical_accuracy: 0.9714\n",
            "Epoch 111/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0416 - categorical_accuracy: 0.9704\n",
            "Epoch 112/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0434 - categorical_accuracy: 0.9704\n",
            "Epoch 113/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0415 - categorical_accuracy: 0.9684\n",
            "Epoch 114/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0436 - categorical_accuracy: 0.9690\n",
            "Epoch 115/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0414 - categorical_accuracy: 0.9739\n",
            "Epoch 116/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0430 - categorical_accuracy: 0.9735\n",
            "Epoch 117/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1641 - categorical_accuracy: 0.9346\n",
            "Epoch 118/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0584 - categorical_accuracy: 0.9653\n",
            "Epoch 119/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0565 - categorical_accuracy: 0.9615\n",
            "Epoch 120/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0435 - categorical_accuracy: 0.9684\n",
            "Epoch 121/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.0411 - categorical_accuracy: 0.9714\n",
            "Epoch 122/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1475 - categorical_accuracy: 0.9353\n",
            "Epoch 123/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.2846 - categorical_accuracy: 0.9150\n",
            "Epoch 124/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0452 - categorical_accuracy: 0.9690\n",
            "Epoch 125/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0463 - categorical_accuracy: 0.9711\n",
            "Epoch 126/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0427 - categorical_accuracy: 0.9714\n",
            "Epoch 127/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0421 - categorical_accuracy: 0.9708\n",
            "Epoch 128/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0429 - categorical_accuracy: 0.9697\n",
            "Epoch 129/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0420 - categorical_accuracy: 0.9725\n",
            "Epoch 130/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0423 - categorical_accuracy: 0.9721\n",
            "Epoch 131/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0425 - categorical_accuracy: 0.9694\n",
            "Epoch 132/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0423 - categorical_accuracy: 0.9721\n",
            "Epoch 133/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0408 - categorical_accuracy: 0.9721\n",
            "Epoch 134/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0412 - categorical_accuracy: 0.9725\n",
            "Epoch 135/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0411 - categorical_accuracy: 0.9732\n",
            "Epoch 136/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0413 - categorical_accuracy: 0.9718\n",
            "Epoch 137/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0404 - categorical_accuracy: 0.9732\n",
            "Epoch 138/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0409 - categorical_accuracy: 0.9694\n",
            "Epoch 139/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.2603 - categorical_accuracy: 0.9112\n",
            "Epoch 140/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.2996 - categorical_accuracy: 0.9061\n",
            "Epoch 141/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1779 - categorical_accuracy: 0.9405\n",
            "Epoch 142/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0484 - categorical_accuracy: 0.9697\n",
            "Epoch 143/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0424 - categorical_accuracy: 0.9766\n",
            "Epoch 144/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0412 - categorical_accuracy: 0.9739\n",
            "Epoch 145/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0425 - categorical_accuracy: 0.9718\n",
            "Epoch 146/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0406 - categorical_accuracy: 0.9704\n",
            "Epoch 147/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0406 - categorical_accuracy: 0.9739\n",
            "Epoch 148/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1825 - categorical_accuracy: 0.9419\n",
            "Epoch 149/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0456 - categorical_accuracy: 0.9773\n",
            "Epoch 150/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0445 - categorical_accuracy: 0.9752\n",
            "Epoch 151/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0423 - categorical_accuracy: 0.9745\n",
            "Epoch 152/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0433 - categorical_accuracy: 0.9725\n",
            "Epoch 153/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0413 - categorical_accuracy: 0.9725\n",
            "Epoch 154/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0367 - categorical_accuracy: 0.9794\n",
            "Epoch 155/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0288 - categorical_accuracy: 0.9804\n",
            "Epoch 156/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0272 - categorical_accuracy: 0.9818\n",
            "Epoch 157/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0270 - categorical_accuracy: 0.9821\n",
            "Epoch 158/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0279 - categorical_accuracy: 0.9804\n",
            "Epoch 159/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0265 - categorical_accuracy: 0.9825\n",
            "Epoch 160/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0270 - categorical_accuracy: 0.9811\n",
            "Epoch 161/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0270 - categorical_accuracy: 0.9825\n",
            "Epoch 162/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0266 - categorical_accuracy: 0.9838\n",
            "Epoch 163/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0274 - categorical_accuracy: 0.9804\n",
            "Epoch 164/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0270 - categorical_accuracy: 0.9811\n",
            "Epoch 165/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.3675 - categorical_accuracy: 0.8892\n",
            "Epoch 166/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0998 - categorical_accuracy: 0.9611\n",
            "Epoch 167/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0442 - categorical_accuracy: 0.9804\n",
            "Epoch 168/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.3045 - categorical_accuracy: 0.9102\n",
            "Epoch 169/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0458 - categorical_accuracy: 0.9714\n",
            "Epoch 170/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0503 - categorical_accuracy: 0.9739\n",
            "Epoch 171/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0427 - categorical_accuracy: 0.9714\n",
            "Epoch 172/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0418 - categorical_accuracy: 0.9776\n",
            "Epoch 173/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0886 - categorical_accuracy: 0.9632\n",
            "Epoch 174/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0406 - categorical_accuracy: 0.9773\n",
            "Epoch 175/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0466 - categorical_accuracy: 0.9687\n",
            "Epoch 176/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0383 - categorical_accuracy: 0.9804\n",
            "Epoch 177/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0406 - categorical_accuracy: 0.9725\n",
            "Epoch 178/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0401 - categorical_accuracy: 0.9766\n",
            "Epoch 179/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0390 - categorical_accuracy: 0.9773\n",
            "Epoch 180/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0346 - categorical_accuracy: 0.9776\n",
            "Epoch 181/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0259 - categorical_accuracy: 0.9828\n",
            "Epoch 182/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.7418 - categorical_accuracy: 0.8094\n",
            "Epoch 183/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1569 - categorical_accuracy: 0.9542\n",
            "Epoch 184/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.2279 - categorical_accuracy: 0.9360\n",
            "Epoch 185/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0584 - categorical_accuracy: 0.9694\n",
            "Epoch 186/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0423 - categorical_accuracy: 0.9725\n",
            "Epoch 187/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0419 - categorical_accuracy: 0.9735\n",
            "Epoch 188/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0401 - categorical_accuracy: 0.9783\n",
            "Epoch 189/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0405 - categorical_accuracy: 0.9763\n",
            "Epoch 190/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.0400 - categorical_accuracy: 0.9752\n",
            "Epoch 191/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0366 - categorical_accuracy: 0.9794\n",
            "Epoch 192/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0369 - categorical_accuracy: 0.9776\n",
            "Epoch 193/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0352 - categorical_accuracy: 0.9800\n",
            "Epoch 194/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0301 - categorical_accuracy: 0.9849\n",
            "Epoch 195/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0270 - categorical_accuracy: 0.9831\n",
            "Epoch 196/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0224 - categorical_accuracy: 0.9883\n",
            "Epoch 197/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 1.8351 - categorical_accuracy: 0.6832\n",
            "Epoch 198/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.5270 - categorical_accuracy: 0.8590\n",
            "Epoch 199/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1279 - categorical_accuracy: 0.9584\n",
            "Epoch 200/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0848 - categorical_accuracy: 0.9680\n",
            "Epoch 201/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0555 - categorical_accuracy: 0.9711\n",
            "Epoch 202/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0506 - categorical_accuracy: 0.9780\n",
            "Epoch 203/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0491 - categorical_accuracy: 0.9756\n",
            "Epoch 204/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0525 - categorical_accuracy: 0.9725\n",
            "Epoch 205/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0481 - categorical_accuracy: 0.9749\n",
            "Epoch 206/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0476 - categorical_accuracy: 0.9745\n",
            "Epoch 207/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0417 - categorical_accuracy: 0.9752\n",
            "Epoch 208/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0405 - categorical_accuracy: 0.9770\n",
            "Epoch 209/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0383 - categorical_accuracy: 0.9766\n",
            "Epoch 210/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0380 - categorical_accuracy: 0.9804\n",
            "Epoch 211/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0341 - categorical_accuracy: 0.9838\n",
            "Epoch 212/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0278 - categorical_accuracy: 0.9838\n",
            "Epoch 213/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0294 - categorical_accuracy: 0.9821\n",
            "Epoch 214/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0284 - categorical_accuracy: 0.9818\n",
            "Epoch 215/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0259 - categorical_accuracy: 0.9842\n",
            "Epoch 216/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0231 - categorical_accuracy: 0.9890\n",
            "Epoch 217/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0246 - categorical_accuracy: 0.9859\n",
            "Epoch 218/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1011 - categorical_accuracy: 0.9601\n",
            "Epoch 219/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0813 - categorical_accuracy: 0.9666\n",
            "Epoch 220/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0271 - categorical_accuracy: 0.9849\n",
            "Epoch 221/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0275 - categorical_accuracy: 0.9842\n",
            "Epoch 222/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0260 - categorical_accuracy: 0.9845\n",
            "Epoch 223/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0258 - categorical_accuracy: 0.9859\n",
            "Epoch 224/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0470 - categorical_accuracy: 0.9804\n",
            "Epoch 225/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.3344 - categorical_accuracy: 0.9088\n",
            "Epoch 226/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0532 - categorical_accuracy: 0.9821\n",
            "Epoch 227/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0297 - categorical_accuracy: 0.9893\n",
            "Epoch 228/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0264 - categorical_accuracy: 0.9880\n",
            "Epoch 229/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0250 - categorical_accuracy: 0.9886\n",
            "Epoch 230/250\n",
            "91/91 [==============================] - 3s 27ms/step - loss: 0.0259 - categorical_accuracy: 0.9842\n",
            "Epoch 231/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0241 - categorical_accuracy: 0.9856\n",
            "Epoch 232/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0279 - categorical_accuracy: 0.9838\n",
            "Epoch 233/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0227 - categorical_accuracy: 0.9859\n",
            "Epoch 234/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0259 - categorical_accuracy: 0.9849\n",
            "Epoch 235/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0227 - categorical_accuracy: 0.9862\n",
            "Epoch 236/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0240 - categorical_accuracy: 0.9893\n",
            "Epoch 237/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0225 - categorical_accuracy: 0.9886\n",
            "Epoch 238/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0216 - categorical_accuracy: 0.9886\n",
            "Epoch 239/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.1074 - categorical_accuracy: 0.9639\n",
            "Epoch 240/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.1106 - categorical_accuracy: 0.9573\n",
            "Epoch 241/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0229 - categorical_accuracy: 0.9897\n",
            "Epoch 242/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0279 - categorical_accuracy: 0.9849\n",
            "Epoch 243/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0231 - categorical_accuracy: 0.9873\n",
            "Epoch 244/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0231 - categorical_accuracy: 0.9883\n",
            "Epoch 245/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0224 - categorical_accuracy: 0.9893\n",
            "Epoch 246/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0210 - categorical_accuracy: 0.9890\n",
            "Epoch 247/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0259 - categorical_accuracy: 0.9873\n",
            "Epoch 248/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0235 - categorical_accuracy: 0.9890\n",
            "Epoch 249/250\n",
            "91/91 [==============================] - 3s 28ms/step - loss: 0.0176 - categorical_accuracy: 0.9900\n",
            "Epoch 250/250\n",
            "91/91 [==============================] - 2s 27ms/step - loss: 0.0226 - categorical_accuracy: 0.9873\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1b88c670990>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=250, callbacks=[tb_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "I5WqPkgTYzsw",
        "outputId": "18a7e3e1-57ae-4945-ac32-09013c6e2b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 30, 64)            82688     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 102)               3366      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 240518 (939.52 KB)\n",
            "Trainable params: 240518 (939.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 9ms/step - loss: 0.0278 - categorical_accuracy: 0.9739\n",
            "5/5 [==============================] - 0s 9ms/step\n",
            "Evaluation Results:\n",
            "Loss: 0.02781902812421322\n",
            "Categorical Accuracy: 0.9738562107086182\n",
            "MAE: 0.00039892589896195913\n",
            "MSE: 0.00018295339763676696\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "eval_result = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Calculate MAE and MSE\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(\"Loss:\", eval_result[0])\n",
        "print(\"Categorical Accuracy:\", eval_result[1])\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWf9cYaaYzsw"
      },
      "source": [
        "# 8. Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "BtCGp-H6Yzsw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 9ms/step\n"
          ]
        }
      ],
      "source": [
        "res = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Fsk0Td_AYzsw",
        "outputId": "fd493d0c-0609-4112-b8a7-70849d8eb179"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Actor'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actions[np.argmax(res[4])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "1MiL83g5Yzsw",
        "outputId": "a1b18ac1-26d9-4b8e-8f99-581b9a6573d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Actor'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actions[np.argmax(y_test[4])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6GSh8wuYzsw"
      },
      "source": [
        "# 9. Save Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "lz_6MoMPYzsw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('action.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCyceITIYzsw"
      },
      "outputs": [],
      "source": [
        "# del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "S0-Dt3tLYzsw"
      },
      "outputs": [],
      "source": [
        "model.load_weights('action.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqYcYv6uYzsw"
      },
      "source": [
        "# 10. Evaluation using Confusion Matrix and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "DxarUwf1Yzsw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "COFsJRWUYzsw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 16ms/step\n"
          ]
        }
      ],
      "source": [
        "yhat = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "R5N6bFnhYzsx"
      },
      "outputs": [],
      "source": [
        "ytrue = np.argmax(y_test, axis=1).tolist()\n",
        "yhat = np.argmax(yhat, axis=1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "9j9gnCGxYzsx",
        "outputId": "b8019d07-a3cf-44e3-b53b-c5ed8611ad87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  0,   4]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  4,   0]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  0,   4]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  0,   4]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  0,   4]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  0,   4]],\n",
              "\n",
              "       [[149,   0],\n",
              "        [  0,   4]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[151,   0],\n",
              "        [  0,   2]],\n",
              "\n",
              "       [[150,   0],\n",
              "        [  0,   3]],\n",
              "\n",
              "       [[148,   4],\n",
              "        [  0,   1]],\n",
              "\n",
              "       [[152,   0],\n",
              "        [  0,   1]]], dtype=int64)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multilabel_confusion_matrix(ytrue, yhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "jB0QJ8X6Yzsx",
        "outputId": "540b61e1-6bbf-4a42-f666-5b427d5ea976"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9738562091503268"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(ytrue, yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8PhLCHOYzsx"
      },
      "source": [
        "# 11. Test in Real Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZH8d7VhYzsx"
      },
      "outputs": [],
      "source": [
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3peXfEDlYzsx"
      },
      "outputs": [],
      "source": [
        "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
        "def prob_viz(res, actions, input_frame, colors):\n",
        "    output_frame = input_frame.copy()\n",
        "    for num, prob in enumerate(res):\n",
        "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
        "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
        "\n",
        "    return output_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wUu_oX5rYzsx",
        "outputId": "45f4932d-03a9-4cda-af3a-00058f07e96c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,18))\n",
        "plt.imshow(prob_viz(res, actions, image, colors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oQo4_jbYzsx"
      },
      "outputs": [],
      "source": [
        "# 1. New detection variables\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.5\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "# Set mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # Read feed\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Make detections\n",
        "        image, results = mediapipe_detection(frame, holistic)\n",
        "        print(results)\n",
        "\n",
        "        # Draw landmarks\n",
        "        draw_styled_landmarks(image, results)\n",
        "\n",
        "        # 2. Prediction logic\n",
        "        keypoints = extract_keypoints(results)\n",
        "        sequence.append(keypoints)\n",
        "        sequence = sequence[-30:]\n",
        "\n",
        "        if len(sequence) == 30:\n",
        "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
        "            print(actions[np.argmax(res)])\n",
        "            predictions.append(np.argmax(res))\n",
        "\n",
        "\n",
        "        #3. Viz logic\n",
        "            if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
        "                if res[np.argmax(res)] > threshold:\n",
        "\n",
        "                    if len(sentence) > 0:\n",
        "                        if actions[np.argmax(res)] != sentence[-1]:\n",
        "                            sentence.append(actions[np.argmax(res)])\n",
        "                    else:\n",
        "                        sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "            if len(sentence) > 5:\n",
        "                sentence = sentence[-5:]\n",
        "\n",
        "            # Viz probabilities\n",
        "            image = prob_viz(res, actions, image, colors)\n",
        "\n",
        "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
        "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Show to screen\n",
        "        cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "        # Break gracefully\n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
